{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training notebook\n",
    "\n",
    "This notebook performs the training of our models.\n",
    "\n",
    "It also includes code to make predictions to submit in the Kaggle challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "import lightgbm as lgb\n",
    "# custom imports\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "import ipdb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import wandb\n",
    "from wandb.lightgbm import wandb_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = 42                        # We want all things\n",
    "# seed_everything(SEED)            # to be as deterministic \n",
    "# lgb_params['seed'] = SEED        # as possible\n",
    "N_CORES = psutil.cpu_count()       # Available CPU cores\n",
    "ver='finalRun'\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            # Our target\n",
    "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1941               # End day of our train set\n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "\n",
    "# weight_features = ['Weight','ScalingFactor','CombinedWeight']\n",
    "# FEATURES to remove: These features lead to overfit or values not present in test set\n",
    "remove_features = ['id','store_id','state_id',\n",
    "                   'date','wm_yr_wk','d',TARGET] #+ weight_features\n",
    "# additional featues we tried to remove\n",
    "# remove_features = remove_features + ['cat_id','event_name_1','event_type_1','event_name_2','event_type_2',\n",
    "#                                     'snap_CA','snap_TX','snap_WI',\n",
    "#                                     'price_momentum','price_nunique','tm_w_end']\n",
    "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
    "                  'enc_dept_id_mean','enc_dept_id_std',\n",
    "                  'enc_item_id_mean','enc_item_id_std']\n",
    "# mean features which we used with running with reduced features\n",
    "#mean_features   = ['enc_item_id_mean','enc_item_id_std']\n",
    "\n",
    "#PATHS for Features\n",
    "#ORIGINAL = '../input/m5-forecasting-accuracy/'\n",
    "BASE     = 'grid_part_1.pkl'\n",
    "PRICE    = 'grid_part_2.pkl'\n",
    "CALENDAR = 'grid_part_3.pkl'\n",
    "LAGS     = 'lags_df_28.pkl'\n",
    "MEAN_ENC = 'mean_encoding_df.pkl'\n",
    "\n",
    "\n",
    "#STORES ids\n",
    "STORES_IDS = pd.read_csv('sales_train_evaluation.csv')['store_id']\n",
    "STORES_IDS = list(STORES_IDS.unique())\n",
    "\n",
    "#SPLITS for lags creation\n",
    "SHIFT_DAY  = 28\n",
    "N_LAGS     = 15\n",
    "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
    "ROLS_SPLIT = []\n",
    "for i in [1,7,14]:\n",
    "    for j in [7,14,30,60]:\n",
    "        ROLS_SPLIT.append([i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiprocess Runs\n",
    "def df_parallelize_run(func, t_split):\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_by_store(store):\n",
    "    \n",
    "    # Read and contact basic feature\n",
    "    df = pd.concat([pd.read_pickle(BASE),\n",
    "                pd.read_pickle(PRICE).iloc[:,2:],\n",
    "                pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
    "                axis=1)\n",
    "\n",
    "    # Leave only relevant store\n",
    "    df = df[df['store_id']==store]\n",
    "    # With memory limits we have to read \n",
    "    # lags and mean encoding features\n",
    "    # separately and drop items that we don't need.\n",
    "    # As our Features Grids are aligned \n",
    "    # we can use index to keep only necessary rows\n",
    "    # Alignment is good for us as concat uses less memory than merge.\n",
    "\n",
    "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "    \n",
    "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "    \n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2 # to not reach memory limit \n",
    "\n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3 # to not reach memory limit \n",
    "    \n",
    "    # Weights featues df to test with weights\n",
    "    #weights_df = df[['id','d']+[col for col in list(df) if col in weight_features]]\n",
    "    # Create features list\n",
    "    features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[['id','d', 'store_id', 'state_id', TARGET]+features]\n",
    "\n",
    "    # Skipping first n rows\n",
    "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
    "\n",
    "    return df, features#, weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recombine Test set after training\n",
    "def get_base_test():\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in STORES_IDS:\n",
    "        temp_df = pd.read_pickle('test_'+store_id+'.pkl')\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lag(LAG_DAY):\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "\n",
    "def make_lag_roll(LAG_DAY):\n",
    "    shift_day = LAG_DAY[0]\n",
    "    roll_wind = LAG_DAY[1]\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
    "    return lag_df[[col_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data and Train the Models\n",
    "To train the model we load the data per store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRMSSE evalutation function to be used in a LGBM model\n",
    "# only computes the WRMSSE score for the lowest level\n",
    "class eval_WRMSSE:\n",
    "    def __init__(self,weights_df):\n",
    "        self.weights_df = weights_df\n",
    "\n",
    "    def eval(self,preds, val_data):\n",
    "#         The weights and the validation set have the same size\n",
    "\n",
    "        labels = val_data.get_label() \n",
    "        Ws = self.weights_df['Weight'].to_numpy()\n",
    "        Ss = self.weights_df['ScalingFactor'].to_numpy()\n",
    "        \n",
    "        RMSSEs = np.sqrt(np.square(labels - preds)/Ss)\n",
    "        WRMSEE = np.sum(Ws * RMSSEs)\n",
    "        \n",
    "        return 'WRMSSE', WRMSEE, False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'None',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'reg_alpha': 1, # l1 regularization \n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 2200,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': -1,\n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CA_1\n",
      "Train CA_2\n",
      "Train CA_3\n",
      "Train CA_4\n",
      "Train TX_1\n",
      "Train TX_2\n",
      "Train TX_3\n",
      "Train WI_1\n",
      "Train WI_2\n",
      "Train WI_3\n"
     ]
    }
   ],
   "source": [
    "total_weights = 0\n",
    "comb_eval_score = 0\n",
    "# for store_id in STORES_IDS:\n",
    "for store_id in STORES_IDS:\n",
    "    print('Train', store_id)\n",
    "    #wandb.init(project='M5_competition')\n",
    "\n",
    "    grid_df, features_columns = get_data_by_store(store_id)\n",
    "\n",
    "    # Masks for \n",
    "    # Train (All data less than 1913)\n",
    "    # \"Validation\" (Last 28 days - not real validatio set) (only used like this for final model)\n",
    "    # Test (All data greater than 1913 day, \n",
    "    #       with some gap for recursive features)\n",
    "    train_mask = grid_df['d']<=END_TRAIN\n",
    "    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
    "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
    "    #weights_df = weights_df.reset_index()\n",
    "    \n",
    "    weighting = dict(zip(np.arange(1,1942), np.arange(1/1941,2,2/1941)))\n",
    "    time_weights = [weighting[d] for d in grid_df[train_mask]['d']]\n",
    "\n",
    "    # Apply masks and save lgb dataset as bin\n",
    "    # to reduce memory spikes during dtype convertations\n",
    "    # https://github.com/Microsoft/LightGBM/issues/1032\n",
    "    # \"To avoid any conversions, you should always use np.float32\"\n",
    "    # or save to bin before start training\n",
    "    # https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53773\n",
    "    \n",
    "    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
    "                       label=grid_df[train_mask][TARGET],\n",
    "                       weight=time_weights)\n",
    "    train_data.save_binary('train_data.bin')\n",
    "    #train_data = lgb.Dataset('train_data.bin')\n",
    "    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
    "                       label=grid_df[valid_mask][TARGET])#,\n",
    "                       #weight=weights_df[valid_mask]['CombinedWeight'])\n",
    "    \n",
    "    # Saving part of the dataset for later predictions\n",
    "    # Removing features that we need to calculate recursively \n",
    "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "    \n",
    "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "    grid_df = grid_df[keep_cols]\n",
    "    grid_df.to_pickle('test_'+store_id+'.pkl')\n",
    "    del grid_df\n",
    "    \n",
    "    # TRAINING\n",
    "    estimator = lgb.train(lgb_params,\n",
    "                          train_data,\n",
    "                          valid_sets = [valid_data],\n",
    "                          verbose_eval = 100\n",
    "                          #feval = eval_WRMSSE(weights_df[valid_mask]).eval, # only used for training previous models\n",
    "                          #callbacks=[wandb_callback()] # write model performance to wandb\n",
    "                          )\n",
    "    \n",
    "    #Adding to the combined eval score\n",
    "    #total_weights += weights_df['Weight'].sum()\n",
    "    #comb_eval_score += estimator.best_score['valid_0']['WRMSSE'] * weights_df['Weight'].sum()\n",
    "\n",
    "    # SAVE MODEL\n",
    "    # - it's not real '.bin' but a pickle file\n",
    "    # estimator = lgb.Booster(model_file='model.txt')\n",
    "    # can only predict with the best iteration (or the saving iteration)\n",
    "    # pickle.dump gives us more flexibility\n",
    "    # like estimator.predict(TEST, num_iteration=100)\n",
    "    # num_iteration - number of iteration want to predict with, \n",
    "    # NULL or <= 0 means use best iteration\n",
    "    model_name = 'lgb_model_'+store_id+'_v'+str(ver)+'.bin'\n",
    "    pickle.dump(estimator, open(model_name, 'wb'))\n",
    "    # Remove temporary files and objects \n",
    "    # to free some hdd space and ram memory\n",
    "    !rm train_data.bin\n",
    "    del train_data, valid_data, estimator\n",
    "    gc.collect()\n",
    "    \n",
    "    # \"Keep\" models features for predictions\n",
    "    MODEL_FEATURES = features_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict | Day: 1\n",
      "> \u001b[0;32m<ipython-input-9-2e9813213b14>\u001b[0m(35)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     34 \u001b[0;31m        \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 35 \u001b[0;31m        \u001b[0mstore_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'store_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mstore_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     36 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-9-2e9813213b14>\u001b[0m(37)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     36 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 37 \u001b[0;31m        \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mday_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     38 \u001b[0;31m        \u001b[0mbase_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTARGET\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMODEL_FEATURES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-9-2e9813213b14>\u001b[0m(38)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     37 \u001b[0;31m        \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mday_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 38 \u001b[0;31m        \u001b[0mbase_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTARGET\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMODEL_FEATURES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     39 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-9-2e9813213b14>\u001b[0m(23)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     22 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 23 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mstore_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTORES_IDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     24 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  base_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    id     d store_id state_id  sales  \\\n",
      "0        HOBBIES_1_001_CA_1_evaluation  1842     CA_1       CA    4.0   \n",
      "1        HOBBIES_1_002_CA_1_evaluation  1842     CA_1       CA    0.0   \n",
      "2        HOBBIES_1_003_CA_1_evaluation  1842     CA_1       CA    1.0   \n",
      "3        HOBBIES_1_004_CA_1_evaluation  1842     CA_1       CA    2.0   \n",
      "4        HOBBIES_1_005_CA_1_evaluation  1842     CA_1       CA    5.0   \n",
      "...                                ...   ...      ...      ...    ...   \n",
      "3902715    FOODS_3_823_WI_3_evaluation  1969     WI_3       WI    NaN   \n",
      "3902716    FOODS_3_824_WI_3_evaluation  1969     WI_3       WI    NaN   \n",
      "3902717    FOODS_3_825_WI_3_evaluation  1969     WI_3       WI    NaN   \n",
      "3902718    FOODS_3_826_WI_3_evaluation  1969     WI_3       WI    NaN   \n",
      "3902719    FOODS_3_827_WI_3_evaluation  1969     WI_3       WI    NaN   \n",
      "\n",
      "               item_id    dept_id   cat_id  release  sell_price  ...  \\\n",
      "0        HOBBIES_1_001  HOBBIES_1  HOBBIES      224    8.257812  ...   \n",
      "1        HOBBIES_1_002  HOBBIES_1  HOBBIES       20    3.970703  ...   \n",
      "2        HOBBIES_1_003  HOBBIES_1  HOBBIES      300    2.970703  ...   \n",
      "3        HOBBIES_1_004  HOBBIES_1  HOBBIES        5    4.640625  ...   \n",
      "4        HOBBIES_1_005  HOBBIES_1  HOBBIES       16    2.880859  ...   \n",
      "...                ...        ...      ...      ...         ...  ...   \n",
      "3902715    FOODS_3_823    FOODS_3    FOODS        0    2.980469  ...   \n",
      "3902716    FOODS_3_824    FOODS_3    FOODS        0    2.480469  ...   \n",
      "3902717    FOODS_3_825    FOODS_3    FOODS        0    3.980469  ...   \n",
      "3902718    FOODS_3_826    FOODS_3    FOODS      230    1.280273  ...   \n",
      "3902719    FOODS_3_827    FOODS_3    FOODS      304    1.000000  ...   \n",
      "\n",
      "         rolling_mean_7  rolling_std_7  rolling_mean_14  rolling_std_14  \\\n",
      "0              0.714355       1.889648         1.000000        1.617188   \n",
      "1              0.142822       0.377930         0.142822        0.363037   \n",
      "2              0.285645       0.488037         0.214233        0.425781   \n",
      "3              1.286133       1.704102         1.142578        1.291992   \n",
      "4              1.142578       1.214844         0.714355        1.069336   \n",
      "...                 ...            ...              ...             ...   \n",
      "3902715        0.571289       0.534668         0.643066        0.841797   \n",
      "3902716        0.285645       0.488037         0.142822        0.363037   \n",
      "3902717        0.856934       0.899902         1.000000        1.109375   \n",
      "3902718        1.857422       2.267578         1.428711        1.650391   \n",
      "3902719        2.714844       1.975586         1.642578        1.823242   \n",
      "\n",
      "         rolling_mean_30  rolling_std_30  rolling_mean_60  rolling_std_60  \\\n",
      "0               0.899902        1.295898         0.750000        1.083008   \n",
      "1               0.600098        1.354492         0.583496        1.109375   \n",
      "2               0.899902        1.184570         1.150391        1.205078   \n",
      "3               1.400391        1.610352         1.616211        1.473633   \n",
      "4               1.099609        1.398438         1.116211        1.276367   \n",
      "...                  ...             ...              ...             ...   \n",
      "3902715         0.633301        0.808594         0.399902        0.717773   \n",
      "3902716         0.300049        0.535156         0.283447        0.523926   \n",
      "3902717         0.766602        0.897461         0.833496        1.028320   \n",
      "3902718         1.366211        1.299805         1.183594        1.213867   \n",
      "3902719         1.166992        1.463867         1.250000        1.642578   \n",
      "\n",
      "         rolling_mean_180  rolling_std_180  \n",
      "0                0.672363         0.932617  \n",
      "1                0.477783         0.874512  \n",
      "2                0.688965         0.964844  \n",
      "3                1.705078         1.979492  \n",
      "4                1.239258         1.220703  \n",
      "...                   ...              ...  \n",
      "3902715          0.605469         0.982910  \n",
      "3902716          0.094421         0.329102  \n",
      "3902717          0.850098         0.959961  \n",
      "3902718          1.216797         1.317383  \n",
      "3902719          1.472656         1.763672  \n",
      "\n",
      "[3902720 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "_,MODEL_FEATURES = get_data_by_store(STORES_IDS[0])\n",
    "\n",
    "# Create Dummy DataFrame to store predictions\n",
    "all_preds = pd.DataFrame()\n",
    "\n",
    "# Join back the Test dataset with a small part of the training data to make recursive features\n",
    "base_test = get_base_test()\n",
    "\n",
    "# Timer to measure predictions time \n",
    "main_time = time.time()\n",
    "\n",
    "# Loop over each prediction day\n",
    "# As rolling lags are the most timeconsuming\n",
    "# we will calculate it for whole day\n",
    "for PREDICT_DAY in range(1,29):    \n",
    "    print('Predict | Day:', PREDICT_DAY)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Make temporary grid to calculate rolling lags\n",
    "    grid_df = base_test.copy()\n",
    "    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n",
    "        \n",
    "    for store_id in STORES_IDS:\n",
    "        \n",
    "        # Read all our models and make predictions\n",
    "        # for each day/store pairs\n",
    "        model_path = 'lgb_model_'+store_id+'_v'+str(ver)+'.bin' \n",
    "        \n",
    "        estimator = pickle.load(open(model_path, 'rb'))\n",
    "        \n",
    "        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY) #1941 + i \n",
    "        ipdb.set_trace()\n",
    "        store_mask = base_test['store_id']==store_id\n",
    "        \n",
    "        mask = (day_mask)&(store_mask)\n",
    "        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
    "        \n",
    "    \n",
    "    # Make good column naming and add \n",
    "    # to all_preds DataFrame\n",
    "    temp_df = base_test[day_mask][['id',TARGET]]\n",
    "    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
    "    if 'id' in list(all_preds):\n",
    "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
    "    else:\n",
    "        all_preds = temp_df.copy()\n",
    "        \n",
    "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
    "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
    "                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
    "    del temp_df\n",
    "    \n",
    "all_preds = all_preds.reset_index(drop=True)\n",
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0.956285</td>\n",
       "      <td>0.958743</td>\n",
       "      <td>0.844446</td>\n",
       "      <td>0.863479</td>\n",
       "      <td>1.002949</td>\n",
       "      <td>1.306109</td>\n",
       "      <td>1.354877</td>\n",
       "      <td>1.112882</td>\n",
       "      <td>0.855429</td>\n",
       "      <td>...</td>\n",
       "      <td>1.051309</td>\n",
       "      <td>1.443870</td>\n",
       "      <td>1.210359</td>\n",
       "      <td>1.091003</td>\n",
       "      <td>0.860383</td>\n",
       "      <td>0.906896</td>\n",
       "      <td>0.974821</td>\n",
       "      <td>1.243794</td>\n",
       "      <td>1.394958</td>\n",
       "      <td>1.189766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>0.220830</td>\n",
       "      <td>0.191704</td>\n",
       "      <td>0.216290</td>\n",
       "      <td>0.196618</td>\n",
       "      <td>0.221648</td>\n",
       "      <td>0.296493</td>\n",
       "      <td>0.356911</td>\n",
       "      <td>0.256091</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291287</td>\n",
       "      <td>0.360629</td>\n",
       "      <td>0.376731</td>\n",
       "      <td>0.242613</td>\n",
       "      <td>0.223980</td>\n",
       "      <td>0.241911</td>\n",
       "      <td>0.249432</td>\n",
       "      <td>0.290434</td>\n",
       "      <td>0.366860</td>\n",
       "      <td>0.385570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>0.549056</td>\n",
       "      <td>0.475419</td>\n",
       "      <td>0.531760</td>\n",
       "      <td>0.493210</td>\n",
       "      <td>0.686122</td>\n",
       "      <td>0.883768</td>\n",
       "      <td>0.746115</td>\n",
       "      <td>0.514638</td>\n",
       "      <td>0.551073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702073</td>\n",
       "      <td>0.738867</td>\n",
       "      <td>0.754876</td>\n",
       "      <td>0.463662</td>\n",
       "      <td>0.413687</td>\n",
       "      <td>0.481749</td>\n",
       "      <td>0.423272</td>\n",
       "      <td>0.577301</td>\n",
       "      <td>0.644268</td>\n",
       "      <td>0.710385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>1.377182</td>\n",
       "      <td>1.347687</td>\n",
       "      <td>1.265495</td>\n",
       "      <td>1.253812</td>\n",
       "      <td>1.721266</td>\n",
       "      <td>2.567457</td>\n",
       "      <td>3.310869</td>\n",
       "      <td>1.939841</td>\n",
       "      <td>1.313381</td>\n",
       "      <td>...</td>\n",
       "      <td>1.810988</td>\n",
       "      <td>2.623314</td>\n",
       "      <td>3.231120</td>\n",
       "      <td>1.688413</td>\n",
       "      <td>1.407650</td>\n",
       "      <td>1.337308</td>\n",
       "      <td>1.379644</td>\n",
       "      <td>1.885810</td>\n",
       "      <td>2.562164</td>\n",
       "      <td>2.965730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>0.990363</td>\n",
       "      <td>1.014008</td>\n",
       "      <td>0.937470</td>\n",
       "      <td>0.869010</td>\n",
       "      <td>1.057996</td>\n",
       "      <td>1.401109</td>\n",
       "      <td>1.476086</td>\n",
       "      <td>1.005547</td>\n",
       "      <td>0.988524</td>\n",
       "      <td>...</td>\n",
       "      <td>1.204729</td>\n",
       "      <td>1.563928</td>\n",
       "      <td>1.587335</td>\n",
       "      <td>1.161014</td>\n",
       "      <td>1.070805</td>\n",
       "      <td>1.093340</td>\n",
       "      <td>1.087575</td>\n",
       "      <td>1.211521</td>\n",
       "      <td>1.586039</td>\n",
       "      <td>1.258535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>FOODS_3_823_WI_3_validation</td>\n",
       "      <td>0.474207</td>\n",
       "      <td>0.493655</td>\n",
       "      <td>0.473603</td>\n",
       "      <td>0.489015</td>\n",
       "      <td>0.492745</td>\n",
       "      <td>0.592487</td>\n",
       "      <td>0.712561</td>\n",
       "      <td>0.527754</td>\n",
       "      <td>0.501123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.604021</td>\n",
       "      <td>0.740740</td>\n",
       "      <td>0.944261</td>\n",
       "      <td>0.650066</td>\n",
       "      <td>0.577998</td>\n",
       "      <td>0.583696</td>\n",
       "      <td>0.492935</td>\n",
       "      <td>0.508034</td>\n",
       "      <td>0.535456</td>\n",
       "      <td>0.684040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>FOODS_3_824_WI_3_validation</td>\n",
       "      <td>0.244012</td>\n",
       "      <td>0.270782</td>\n",
       "      <td>0.243153</td>\n",
       "      <td>0.210993</td>\n",
       "      <td>0.220497</td>\n",
       "      <td>0.254603</td>\n",
       "      <td>0.292987</td>\n",
       "      <td>0.251368</td>\n",
       "      <td>0.216406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238860</td>\n",
       "      <td>0.397177</td>\n",
       "      <td>0.412006</td>\n",
       "      <td>0.309413</td>\n",
       "      <td>0.376216</td>\n",
       "      <td>0.406834</td>\n",
       "      <td>0.272891</td>\n",
       "      <td>0.233452</td>\n",
       "      <td>0.254938</td>\n",
       "      <td>0.337766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>FOODS_3_825_WI_3_validation</td>\n",
       "      <td>0.584675</td>\n",
       "      <td>0.463950</td>\n",
       "      <td>0.418996</td>\n",
       "      <td>0.392635</td>\n",
       "      <td>0.474074</td>\n",
       "      <td>0.492743</td>\n",
       "      <td>0.647157</td>\n",
       "      <td>0.619673</td>\n",
       "      <td>0.418042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776835</td>\n",
       "      <td>1.056976</td>\n",
       "      <td>1.187486</td>\n",
       "      <td>1.028814</td>\n",
       "      <td>0.925572</td>\n",
       "      <td>0.895240</td>\n",
       "      <td>0.692968</td>\n",
       "      <td>0.600376</td>\n",
       "      <td>0.664559</td>\n",
       "      <td>0.742776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>FOODS_3_826_WI_3_validation</td>\n",
       "      <td>0.972861</td>\n",
       "      <td>1.146929</td>\n",
       "      <td>1.024173</td>\n",
       "      <td>0.882643</td>\n",
       "      <td>1.014146</td>\n",
       "      <td>1.161431</td>\n",
       "      <td>1.078507</td>\n",
       "      <td>1.325431</td>\n",
       "      <td>1.204841</td>\n",
       "      <td>...</td>\n",
       "      <td>1.087420</td>\n",
       "      <td>1.513481</td>\n",
       "      <td>1.521882</td>\n",
       "      <td>1.372155</td>\n",
       "      <td>1.597349</td>\n",
       "      <td>1.433423</td>\n",
       "      <td>1.111526</td>\n",
       "      <td>1.189550</td>\n",
       "      <td>1.403955</td>\n",
       "      <td>1.422572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30489</th>\n",
       "      <td>FOODS_3_827_WI_3_validation</td>\n",
       "      <td>1.862830</td>\n",
       "      <td>1.720095</td>\n",
       "      <td>1.711895</td>\n",
       "      <td>1.725727</td>\n",
       "      <td>1.982621</td>\n",
       "      <td>2.097581</td>\n",
       "      <td>1.856721</td>\n",
       "      <td>1.820093</td>\n",
       "      <td>1.798293</td>\n",
       "      <td>...</td>\n",
       "      <td>1.485093</td>\n",
       "      <td>2.015198</td>\n",
       "      <td>1.799755</td>\n",
       "      <td>1.843362</td>\n",
       "      <td>2.209127</td>\n",
       "      <td>1.762874</td>\n",
       "      <td>1.651491</td>\n",
       "      <td>1.734956</td>\n",
       "      <td>1.864456</td>\n",
       "      <td>2.195524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30490 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id        F1        F2        F3        F4  \\\n",
       "0      HOBBIES_1_001_CA_1_validation  0.956285  0.958743  0.844446  0.863479   \n",
       "1      HOBBIES_1_002_CA_1_validation  0.220830  0.191704  0.216290  0.196618   \n",
       "2      HOBBIES_1_003_CA_1_validation  0.549056  0.475419  0.531760  0.493210   \n",
       "3      HOBBIES_1_004_CA_1_validation  1.377182  1.347687  1.265495  1.253812   \n",
       "4      HOBBIES_1_005_CA_1_validation  0.990363  1.014008  0.937470  0.869010   \n",
       "...                              ...       ...       ...       ...       ...   \n",
       "30485    FOODS_3_823_WI_3_validation  0.474207  0.493655  0.473603  0.489015   \n",
       "30486    FOODS_3_824_WI_3_validation  0.244012  0.270782  0.243153  0.210993   \n",
       "30487    FOODS_3_825_WI_3_validation  0.584675  0.463950  0.418996  0.392635   \n",
       "30488    FOODS_3_826_WI_3_validation  0.972861  1.146929  1.024173  0.882643   \n",
       "30489    FOODS_3_827_WI_3_validation  1.862830  1.720095  1.711895  1.725727   \n",
       "\n",
       "             F5        F6        F7        F8        F9  ...       F19  \\\n",
       "0      1.002949  1.306109  1.354877  1.112882  0.855429  ...  1.051309   \n",
       "1      0.221648  0.296493  0.356911  0.256091  0.217043  ...  0.291287   \n",
       "2      0.686122  0.883768  0.746115  0.514638  0.551073  ...  0.702073   \n",
       "3      1.721266  2.567457  3.310869  1.939841  1.313381  ...  1.810988   \n",
       "4      1.057996  1.401109  1.476086  1.005547  0.988524  ...  1.204729   \n",
       "...         ...       ...       ...       ...       ...  ...       ...   \n",
       "30485  0.492745  0.592487  0.712561  0.527754  0.501123  ...  0.604021   \n",
       "30486  0.220497  0.254603  0.292987  0.251368  0.216406  ...  0.238860   \n",
       "30487  0.474074  0.492743  0.647157  0.619673  0.418042  ...  0.776835   \n",
       "30488  1.014146  1.161431  1.078507  1.325431  1.204841  ...  1.087420   \n",
       "30489  1.982621  2.097581  1.856721  1.820093  1.798293  ...  1.485093   \n",
       "\n",
       "            F20       F21       F22       F23       F24       F25       F26  \\\n",
       "0      1.443870  1.210359  1.091003  0.860383  0.906896  0.974821  1.243794   \n",
       "1      0.360629  0.376731  0.242613  0.223980  0.241911  0.249432  0.290434   \n",
       "2      0.738867  0.754876  0.463662  0.413687  0.481749  0.423272  0.577301   \n",
       "3      2.623314  3.231120  1.688413  1.407650  1.337308  1.379644  1.885810   \n",
       "4      1.563928  1.587335  1.161014  1.070805  1.093340  1.087575  1.211521   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "30485  0.740740  0.944261  0.650066  0.577998  0.583696  0.492935  0.508034   \n",
       "30486  0.397177  0.412006  0.309413  0.376216  0.406834  0.272891  0.233452   \n",
       "30487  1.056976  1.187486  1.028814  0.925572  0.895240  0.692968  0.600376   \n",
       "30488  1.513481  1.521882  1.372155  1.597349  1.433423  1.111526  1.189550   \n",
       "30489  2.015198  1.799755  1.843362  2.209127  1.762874  1.651491  1.734956   \n",
       "\n",
       "            F27       F28  \n",
       "0      1.394958  1.189766  \n",
       "1      0.366860  0.385570  \n",
       "2      0.644268  0.710385  \n",
       "3      2.562164  2.965730  \n",
       "4      1.586039  1.258535  \n",
       "...         ...       ...  \n",
       "30485  0.535456  0.684040  \n",
       "30486  0.254938  0.337766  \n",
       "30487  0.664559  0.742776  \n",
       "30488  1.403955  1.422572  \n",
       "30489  1.864456  2.195524  \n",
       "\n",
       "[30490 rows x 29 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds['id'] = all_preds['id'].map(lambda x: x.replace('evaluation','validation'))\n",
    "all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Kaggle sumbission notebook for public leaderbord only\n",
    "ORIGINAL = ''\n",
    "\n",
    "# Reading competition sample submission and\n",
    "# merging our predictions\n",
    "# As we have predictions only for \"_validation\" data\n",
    "# we need to do fillna() for \"_evaluation\" items\n",
    "submission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\n",
    "submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n",
    "submission.to_csv('submission_v'+str(ver)+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
